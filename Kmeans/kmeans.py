import numpy as np
import os
import matplotlib
import matplotlib.pyplot as plt

plt.rcParams['axes.labelsize'] = 14
plt.rcParams['xtick.labelsize'] = 12
plt.rcParams['ytick.labelsize'] = 12
import warnings

warnings.filterwarnings('ignore')
np.random.seed(42)

from sklearn.datasets import make_blobs

blob_centers = np.array(
    [[0.2, 2.3],
     [-1.5, 2.3],
     [-2.8, 1.8],
     [-2.8, 2.8],
     [-2.8, 1.3]])

blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])

X, y = make_blobs(n_samples=2000, centers=blob_centers,
                  cluster_std=blob_std, random_state=7)


def plot_clusters(X, y=None):
    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)
    plt.xlabel("$x_1$", fontsize=14)
    plt.ylabel("$x_2$", fontsize=14, rotation=0)


plt.figure(figsize=(8, 4))
plot_clusters(X)
plt.show()

from sklearn.cluster import KMeans

k = 5
kmeans = KMeans(n_clusters=k, random_state=42)
y_pred = kmeans.fit_predict(X)

print(y_pred)

print(kmeans.labels_)
print(kmeans.cluster_centers_)

X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])
kmeans.predict(X_new)

print(kmeans.transform(X_new))


# èšç±»ç»“æœå±•ç¤º
def plot_data(X):
    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)


# ç”»ä¸­å¿ƒç‚¹
def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):
    if weights is not None:
        centroids = centroids[weights > weights.max() / 10]
    plt.scatter(centroids[:, 0], centroids[:, 1],
                marker='o', s=30, linewidths=8,
                color=circle_color, zorder=10, alpha=0.9)
    plt.scatter(centroids[:, 0], centroids[:, 1],
                marker='x', s=50, linewidths=50,
                color=cross_color, zorder=11, alpha=1)


def plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,
                             show_xlabels=True, show_ylabels=True):
    mins = X.min(axis=0) - 0.1
    maxs = X.max(axis=0) + 0.1
    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),
                         np.linspace(mins[1], maxs[1], resolution))
    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)

    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),
                 cmap="Pastel2")
    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]),
                linewidths=1, colors='k')
    plot_data(X)
    if show_centroids:
        plot_centroids(clusterer.cluster_centers_)

    if show_xlabels:
        plt.xlabel("$x_1$", fontsize=14)
    else:
        plt.tick_params(labelbottom='off')
    if show_ylabels:
        plt.ylabel("$x_2$", fontsize=14, rotation=0)
    else:
        plt.tick_params(labelleft='off')


plt.figure(figsize=(8, 4))
plot_decision_boundaries(kmeans, X)
plt.show()

# ç®—æ³•æµç¨‹
kmeans_iter1 = KMeans(n_clusters=5, init='random', n_init=1, max_iter=1, random_state=1)
kmeans_iter2 = KMeans(n_clusters=5, init='random', n_init=1, max_iter=2, random_state=1)
kmeans_iter3 = KMeans(n_clusters=5, init='random', n_init=1, max_iter=3, random_state=1)

kmeans_iter1.fit(X)
kmeans_iter2.fit(X)
kmeans_iter3.fit(X)

plt.figure(figsize=(12, 8))
plt.subplot(321)
plot_data(X)
plot_centroids(kmeans_iter1.cluster_centers_, circle_color='r', cross_color='k')
plt.title('Update cluster_centers')

plt.subplot(322)
plot_decision_boundaries(kmeans_iter1, X, show_xlabels=False, show_ylabels=False)
plt.title('Label')

plt.subplot(323)
plot_decision_boundaries(kmeans_iter1, X, show_xlabels=False, show_ylabels=False)
plot_centroids(kmeans_iter2.cluster_centers_, )

plt.subplot(324)
plot_decision_boundaries(kmeans_iter2, X, show_xlabels=False, show_ylabels=False)

plt.subplot(325)
plot_decision_boundaries(kmeans_iter2, X, show_xlabels=False, show_ylabels=False)
plot_centroids(kmeans_iter3.cluster_centers_, )

plt.subplot(326)
plot_decision_boundaries(kmeans_iter3, X, show_xlabels=False, show_ylabels=False)

plt.show()


# ä¸ç¨³å®šçš„ç»“æœ
def plot_clusterer_comparison(c1, c2, X):
    c1.fit(X)
    c2.fit(X)

    plt.figure(figsize=(12, 4))
    plt.subplot(121)
    plot_decision_boundaries(c1, X)
    plt.subplot(122)
    plot_decision_boundaries(c2, X)


c1 = KMeans(n_clusters=5, init='random', n_init=1, random_state=11)
c2 = KMeans(n_clusters=5, init='random', n_init=1, random_state=19)
plot_clusterer_comparison(c1, c2, X)

# è¯„ä¼°æ–¹æ³•: InertiaæŒ‡æ ‡ï¼šæ¯ä¸ªæ ·æœ¬ä¸å…¶è´¨å¿ƒçš„è·ç¦»
kmeans.inertia_
X_dist = kmeans.transform(X)
kmeans.transform(X)
kmeans.labels_
X_dist[np.arange(len(X_dist)), kmeans.labels_]
np.sum(X_dist[np.arange(len(X_dist)), kmeans.labels_] ** 2)
kmeans.score(X)
c1.inertia_
c2.inertia_

# æ‰¾åˆ°æœ€ä½³ç°‡æ•°
# å¦‚æœkå€¼è¶Šå¤§ï¼Œå¾—åˆ°çš„ç»“æœè‚¯å®šä¼šè¶Šæ¥è¶Šå°ï¼ï¼ï¼
kmeans_per_k = [KMeans(n_clusters=k).fit(X) for k in range(1, 10)]
inertias = [model.inertia_ for model in kmeans_per_k]
plt.figure(figsize=(8, 4))
plt.plot(range(1, 10), inertias, 'bo-')
plt.axis([1, 8.5, 0, 1300])
plt.show()

# è½®å»“ç³»æ•°
# ğ‘ğ‘– : è®¡ç®—æ ·æœ¬iåˆ°åŒç°‡å…¶ä»–æ ·æœ¬çš„å¹³å‡è·ç¦»aiã€‚ai è¶Šå°ï¼Œè¯´æ˜æ ·æœ¬iè¶Šåº”è¯¥è¢«èšç±»åˆ°è¯¥ç°‡ã€‚å°†ai ç§°ä¸ºæ ·æœ¬içš„ç°‡å†…ä¸ç›¸ä¼¼åº¦ã€‚
# ğ‘ğ‘– : è®¡ç®—æ ·æœ¬iåˆ°å…¶ä»–æŸç°‡Cj çš„æ‰€æœ‰æ ·æœ¬çš„å¹³å‡è·ç¦»bijï¼Œç§°ä¸ºæ ·æœ¬iä¸ç°‡Cj çš„ä¸ç›¸ä¼¼åº¦ã€‚
# å®šä¹‰ä¸ºæ ·æœ¬içš„ç°‡é—´ä¸ç›¸ä¼¼åº¦ï¼šbi =min{bi1, bi2, ..., bik}

from sklearn.metrics import silhouette_score

silhouette_score(X, kmeans.labels_)
print(kmeans_per_k)

# labels is 2 <= n_labels <= n_samples - 1.
silhouette_scores = [silhouette_score(X, model.labels_) for model in kmeans_per_k[1:]]
print(silhouette_scores)

plt.figure(figsize=(8, 4))
plt.plot(range(2, 10), silhouette_scores, 'bo-')
plt.show()

# Kmeanså­˜åœ¨çš„é—®é¢˜
X1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)
X1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))
X2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)
X2 = X2 + [6, -8]
X = np.r_[X1, X2]
y = np.r_[y1, y2]

plot_data(X)

kmeans_good = KMeans(n_clusters=3, init=np.array([[-1.5, 2.5], [0.5, 0], [4, 0]]), n_init=1, random_state=42)
kmeans_bad = KMeans(n_clusters=3, random_state=42)
kmeans_good.fit(X)
kmeans_bad.fit(X)

plt.figure(figsize=(10, 4))
plt.subplot(121)
plot_decision_boundaries(kmeans_good, X)
plt.title('Good - inertia = {}'.format(kmeans_good.inertia_))

plt.subplot(122)
plot_decision_boundaries(kmeans_bad, X)
plt.title('Bad - inertia = {}'.format(kmeans_bad.inertia_))

# å›¾åƒåˆ†å‰²å°ä¾‹å­
# ladybug.png
from matplotlib.image import imread

image = imread('ladybug.png')
image.shape

X = image.reshape(-1, 3)
X.shape

kmeans = KMeans(n_clusters=8, random_state=42).fit(X)
print(kmeans.cluster_centers_)

segmented_img = kmeans.cluster_centers_[kmeans.labels_].reshape(533, 800, 3)

segmented_imgs = []
n_colors = (10,8,6,4,2)
for n_cluster in n_colors:
    kmeans = KMeans(n_clusters = n_cluster,random_state=42).fit(X)
    segmented_img = kmeans.cluster_centers_[kmeans.labels_]
    segmented_imgs.append(segmented_img.reshape(image.shape))

plt.figure(figsize=(10,5))
plt.subplot(231)
plt.imshow(image)
plt.title('Original image')

for idx,n_clusters in enumerate(n_colors):
    plt.subplot(232+idx)
    plt.imshow(segmented_imgs[idx])
    plt.title('{}colors'.format(n_clusters))


# åŠç›‘ç£å­¦ä¹ 
# é¦–å…ˆï¼Œè®©æˆ‘ä»¬å°†è®­ç»ƒé›†èšç±»ä¸º50ä¸ªé›†ç¾¤ï¼Œ ç„¶åå¯¹äºæ¯ä¸ªèšç±»ï¼Œè®©æˆ‘ä»¬æ‰¾åˆ°æœ€é è¿‘è´¨å¿ƒçš„å›¾åƒã€‚ æˆ‘ä»¬å°†è¿™äº›å›¾åƒç§°ä¸ºä»£è¡¨æ€§å›¾åƒï¼š
from sklearn.datasets import load_digits

X_digits,y_digits = load_digits(return_X_y = True)

from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(X_digits,y_digits,random_state=42)

print(y_train.shape)


from sklearn.linear_model import LogisticRegression
n_labeled = 50

log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_train[:n_labeled], y_train[:n_labeled])
log_reg.score(X_test, y_test)

k = 50
kmeans = KMeans(n_clusters=k, random_state=42)
X_digits_dist = kmeans.fit_transform(X_train)

print(X_digits_dist.shape)

representative_digits_idx = np.argmin(X_digits_dist,axis=0)
print(representative_digits_idx.shape)

X_representative_digits = X_train[representative_digits_idx]

# ç°åœ¨è®©æˆ‘ä»¬ç»˜åˆ¶è¿™äº›ä»£è¡¨æ€§å›¾åƒå¹¶æ‰‹åŠ¨æ ‡è®°å®ƒä»¬ï¼š
plt.figure(figsize=(8, 2))
for index, X_representative_digit in enumerate(X_representative_digits):
    plt.subplot(k // 10, 10, index + 1)
    plt.imshow(X_representative_digit.reshape(8, 8), cmap="binary", interpolation="bilinear")
    plt.axis('off')

plt.show()

y_representative_digits = np.array([
    4, 8, 0, 6, 8, 3, 7, 7, 9, 2,
    5, 5, 8, 5, 2, 1, 2, 9, 6, 1,
    1, 6, 9, 0, 8, 3, 0, 7, 4, 1,
    6, 5, 2, 4, 1, 8, 6, 3, 9, 2,
    4, 2, 9, 4, 7, 6, 2, 3, 1, 1])

# ç°åœ¨æˆ‘ä»¬æœ‰ä¸€ä¸ªåªæœ‰50ä¸ªæ ‡è®°å®ä¾‹çš„æ•°æ®é›†ï¼Œå®ƒä»¬ä¸­çš„æ¯ä¸€ä¸ªéƒ½æ˜¯å…¶é›†ç¾¤çš„ä»£è¡¨æ€§å›¾åƒï¼Œè€Œä¸æ˜¯å®Œå…¨éšæœºçš„å®ä¾‹ã€‚ è®©æˆ‘ä»¬çœ‹çœ‹æ€§èƒ½æ˜¯å¦æ›´å¥½ï¼š
log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_representative_digits, y_representative_digits)
log_reg.score(X_test, y_test)

# ä½†ä¹Ÿè®¸æˆ‘ä»¬å¯ä»¥æ›´è¿›ä¸€æ­¥ï¼šå¦‚æœæˆ‘ä»¬å°†æ ‡ç­¾ä¼ æ’­åˆ°åŒä¸€ç¾¤é›†ä¸­çš„æ‰€æœ‰å…¶ä»–å®ä¾‹ï¼Œè¯¥æ€ä¹ˆåŠï¼Ÿ
y_train_propagated = np.empty(len(X_train), dtype=np.int32)
for i in range(k):
    y_train_propagated[kmeans.labels_ == i] = y_representative_digits[i]

log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_train, y_train_propagated)

print(log_reg.score(X_test, y_test))

# åªé€‰æ‹©å‰20ä¸ªæ¥è¯•è¯•
percentile_closest = 20

X_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]
for i in range(k):
    in_cluster = (kmeans.labels_ == i)
    cluster_dist = X_cluster_dist[in_cluster] #é€‰æ‹©å±äºå½“å‰ç°‡çš„æ‰€æœ‰æ ·æœ¬
    cutoff_distance = np.percentile(cluster_dist, percentile_closest) #æ’åºæ‰¾åˆ°å‰20ä¸ª
    above_cutoff = (X_cluster_dist > cutoff_distance) # False Trueç»“æœ
    X_cluster_dist[in_cluster & above_cutoff] = -1

partially_propagated = (X_cluster_dist != -1)
X_train_partially_propagated = X_train[partially_propagated]
y_train_partially_propagated = y_train_propagated[partially_propagated]

log_reg = LogisticRegression(random_state=42)
log_reg.fit(X_train_partially_propagated, y_train_partially_propagated)

print(log_reg.score(X_test, y_test))



# DBSCAN
from sklearn.datasets import make_moons
X, y = make_moons(n_samples=1000, noise=0.05, random_state=42)
plt.plot(X[:,0],X[:,1],'b.')

from sklearn.cluster import DBSCAN
dbscan = DBSCAN(eps = 0.05,min_samples=5)
dbscan.fit(X)

# å¸¸ç”¨å±æ€§
print(dbscan.labels_[:10])

print(dbscan.core_sample_indices_[:10])

print(np.unique(dbscan.labels_))

dbscan2 = DBSCAN(eps = 0.2,min_samples=5)
dbscan2.fit(X)

# ç”»å›¾
def plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):
    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)
    core_mask[dbscan.core_sample_indices_] = True
    anomalies_mask = dbscan.labels_ == -1
    non_core_mask = ~(core_mask | anomalies_mask)

    cores = dbscan.components_
    anomalies = X[anomalies_mask]
    non_cores = X[non_core_mask]

    plt.scatter(cores[:, 0], cores[:, 1],
                c=dbscan.labels_[core_mask], marker='o', s=size, cmap="Paired")
    plt.scatter(cores[:, 0], cores[:, 1], marker='*', s=20, c=dbscan.labels_[core_mask])
    plt.scatter(anomalies[:, 0], anomalies[:, 1],
                c="r", marker="x", s=100)
    plt.scatter(non_cores[:, 0], non_cores[:, 1], c=dbscan.labels_[non_core_mask], marker=".")
    if show_xlabels:
        plt.xlabel("$x_1$", fontsize=14)
    else:
        plt.tick_params(labelbottom='off')
    if show_ylabels:
        plt.ylabel("$x_2$", fontsize=14, rotation=0)
    else:
        plt.tick_params(labelleft='off')
    plt.title("eps={:.2f}, min_samples={}".format(dbscan.eps, dbscan.min_samples), fontsize=14)

plt.figure(figsize=(9, 3.2))

plt.subplot(121)
plot_dbscan(dbscan, X, size=100)

plt.subplot(122)
plot_dbscan(dbscan2, X, size=600, show_ylabels=False)

plt.show()









